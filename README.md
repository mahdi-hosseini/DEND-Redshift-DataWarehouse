# DEND-Redshift-DataWarehouse

## 1. Summary and purpose

Sparkify wants to put the data into the hands of data analysts to be able to get a better understanding of user demographics, listening profile and potentially boost conversions and reduce churn. Currently the data is stored in log files that are not queriable and not accessbile for analysis.

The purpose of this workflow project is to perform dimensional modeling and store the transformed data into a data warehouse so that OLAP queries could be run against it. In order to faciliate that, we need to get the data out of its current normalized form that is suited for OLTP setting and load it in an Amazon Redshift datawarehouse.


## 2. Dataset Description

The dataset for this project consists of music streaming activity log and and song dataset. This data resides in a S3 bucket named `udacity-dend`.

### a. Activity log data

**Location**: `s3://udacity-dend/log-data`

Simulated Music streaming app activity logs (created using [event simulator](https://github.com/Interana/eventsim)) partitioned by year and month.

### b. Song data

**Location**: `s3://udacity-dend/song-data`

Subset of [Million Song Dataset](https://labrosa.ee.columbia.edu/millionsong/) partitioned by the first three letters of each song's track ID.



## 3. Schema Design

The schema design uses star scheme and the act of listening is our fact (songplays table) which is additive and each one is constituted as a single user activity. Dimension tables are users, songs, artists, time. This would enable a data analyst/engineer/scientist to perform minimal number of joins to do their analysis and get results faster.


## 4. ETL pipeline

The Extract-Transfrom-Load (ETL) pipline focuses on active users who have interacted with the streaming platfrom and a filter is applied only on records that have `page = 'NextSong'`. The pipeline steps are as follows:

### a. Extract
1. Use Redshift's `COPY` command to load activity log data into a staging table `staging_events`
2. Use Redshift's `COPY` command to load song data into a staging table `staging_songs`

### b. Transform + Load

In all the tranformation queries, Common Table Expressions (CTE) with `ROW_NUMBER` Window Function under temporal (descending), lexical and/or numerical ordering are used to de-duplicate data by only getting the first occurence of each unique entry. This is done based on a na√Øve assumption and the correct semantics of this must be dececided per business requirements.

1. Use `staging_events` table data to insert new users into `users` dimension table
2. Use `staging_events` table data to insert song data into `songs` dimension table
3. Use `staging_events` table data to insert artist data into `artists` dimension table
4. Use `staging_events` table data to insert timestamps into `time` dimension table
5. Use `staging_events` table data to update existing users' information to the latest changes
6. Use `staging_events` table data to update existing songs' information to the latest changes
7. Use `staging_events` table data to update existing artists' information to the latest changes
8. Use `staging_events` and join with `songs` and `artists` table to insert songplay data into `songplays` fact table


## 5. Project Structure
#### README.md

Description of the project and explanation of the ETL pipeline

#### config/dwh.cfg

Python `ConfigParser` INI file hosting the Amazon Redshift Cluster information, Amazon Redshift IAM Role ARN, and S3 bucket names and flat files manifest file

#### manifests

The dataset file names do not contain a common prefix and `logdata.manifest.json` and `songdata.manifest.json` are used for Redshift COPY command to perform distributed bulk insertion of the dataset. These files are generated by a custom script (not included in this project).


#### create_table.py

Python script to perform creation of all necessary tables


#### etl.py

Performs the steps mentioned in section 4 (ETL) of this document


#### pipeline.py

A Python module containing `ETL` class definition used across all Python scripts as well as helper function for reading the project configuration.


#### sql_queries.py

Contains all DDL (Data Definition Language) and DML (Data Manipulation Language) SQL queries required for running the ETL pipeline.

## 6. How To Run

**Prerequisites**:
* Python 3.7
* PIP packages: Boto3, psycopg2

1. In terminal, navigate to the project root folder

2. In terminal, run `python create_tables.py`

3. In terminal, run `python etl.py`
